{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a789c7-fd0e-43c8-ad71-d820ab7dfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q kaggle\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db164b8b-bfa0-4a22-8f67-19f4d3420be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy torch torchvision transformers tqdm scikit-learn pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54589b6-2bc4-4077-89ea-360c34cecb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `DinoV3` has been saved to /home/jef9921/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/jef9921/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `DinoV3`\n"
     ]
    }
   ],
   "source": [
    "!hf auth login --token <InsertHuggingfaceToken>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4fd9b-f443-40bc-aac9-b74988e97d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from transformers import CLIPVisionModel, AutoModel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "\n",
    "# Suppress minor warnings from transformers\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a729656c-4bed-4de9-861f-9171006dbbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geoguessr Data folder already exists.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"kaggle_dataset\")\n",
    "TRAIN_DIR = BASE_DIR / \"train_images\"\n",
    "TEST_DIR   = BASE_DIR / \"test_images\"\n",
    "# Download data\n",
    "if not BASE_DIR.exists() or not any(BASE_DIR.iterdir()):\n",
    "    print(\"Downloading and unzipping data...\")\n",
    "    os.makedirs(BASE_DIR, exist_ok=True)\n",
    "    print(\"Downloading...\")\n",
    "    !kaggle competitions download -c geo-guessr-street-view-cs-gy-6643\n",
    "    print(\"Unzipping...\")\n",
    "    !unzip -q geo-guessr-street-view-cs-gy-6643.zip\n",
    "else:\n",
    "    print(\"Geoguessr Data folder already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b694793-5fd0-4c5f-9597-557a7ee26d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    # Paths\n",
    "    TRAIN_CSV = BASE_DIR / \"train_ground_truth.csv\"\n",
    "    TEST_CSV = BASE_DIR / \"sample_submission.csv\"\n",
    "    TRAIN_IMG_DIR = BASE_DIR / \"train_images/\"\n",
    "    TEST_IMG_DIR = BASE_DIR / \"test_images/\"\n",
    "    \n",
    "    SUBMISSION_FILE = \"test_submission_35_epochs_knn_tta.csv\" \n",
    "\n",
    "    RESUME_PATH = \"best_model_full.pth\"  # Might need to change to \"best_model_full.pth\"\n",
    "    \n",
    "    SAVE_DIR = \"checkpoints\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Model Hyperparameters\n",
    "    MODEL_NAME_CLIP = \"geolocal/StreetCLIP\"\n",
    "    MODEL_NAME_DINO = \"facebook/dinov3-vitl16-pretrain-lvd1689m\"\n",
    "    IMG_SIZE = 336\n",
    "    BATCH_SIZE = 4        \n",
    "    NUM_WORKERS = 4\n",
    "    \n",
    "    # KEY CHANGES FOR 35 EPOCHS\n",
    "    EPOCHS = 35           # Changed from 10\n",
    "               \n",
    "    LR = 1e-7             \n",
    "    \n",
    "    # Increase Weight Decay to prevent overfitting\n",
    "    WD = 1e-2             # Increased from 1e-4\n",
    "    \n",
    "    # Loss Weights\n",
    "    W_CLS = 1.0\n",
    "    W_GPS = 15.0          # Slightly increased to emphasize GPS precision in late training\n",
    "    \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94f034",
   "metadata": {},
   "source": [
    "For this part, we define utilities that are directly tied to how the model is trained and evaluated, along with the dataset class that structures the input data.\n",
    "\n",
    "We first implement the Haversine distance function, which computes the true geographic distance between two latitude longitude pairs in kilometers. We use this instead of simple Euclidean distance because the Earth is spherical, and this metric matches the GPS evaluation used in the competition. During training, it provides an interpretable measure of how far predictions are from the ground truth in real-world units.\n",
    "\n",
    "Next, we define a GPS normalization helper. Raw latitude and longitude values span different numeric ranges and scales, which makes direct regression unstable. We normalize both coordinates using fixed means and scales so that the regression head learns in a more balanced and well-conditioned space. At inference time, the predictions are converted back to real GPS coordinates using the inverse transformation.\n",
    "\n",
    "Finally, we implement a custom dataset class for the Street View data. Each sample consists of four images captured at the same location, facing north, east, south, and west. We load all four images, apply the same transformations, and stack them into a single tensor so the model can reason jointly over multiple directions.\n",
    "\n",
    "If an image fails to load, we substitute a black image to avoid breaking the training loop while keeping tensor shapes consistent. The dataset supports both training and test modes. In training mode, it returns the stacked images along with state and GPS labels. In test mode, it returns only the images and the sample identifier, which is required for submission generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d164cbf9-ca09-4dcf-a590-5d25df2a8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. UTILS & DATASET\n",
    "# ==========================================\n",
    "def haversine_distance(pred_lat, pred_lon, true_lat, true_lon):\n",
    "    \"\"\"Calculates Haversine distance in km.\"\"\"\n",
    "    R = 6371\n",
    "    phi1, phi2 = torch.deg2rad(pred_lat), torch.deg2rad(true_lat)\n",
    "    dphi = torch.deg2rad(true_lat - pred_lat)\n",
    "    dlambda = torch.deg2rad(true_lon - pred_lon)\n",
    "    a = torch.sin(dphi/2)**2 + torch.cos(phi1)*torch.cos(phi2)*torch.sin(dlambda/2)**2\n",
    "    a = torch.clamp(a, 0, 1)\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "class GPSNormalizer:\n",
    "    def __init__(self):\n",
    "        self.lat_mean, self.lat_scale = 37.0, 15.0\n",
    "        self.lon_mean, self.lon_scale = -95.0, 30.0\n",
    "\n",
    "    def normalize(self, lat, lon):\n",
    "        return (lat - self.lat_mean)/self.lat_scale, (lon - self.lon_mean)/self.lon_scale\n",
    "\n",
    "    def denormalize(self, n_lat, n_lon):\n",
    "        return (n_lat * self.lat_scale) + self.lat_mean, (n_lon * self.lon_scale) + self.lon_mean\n",
    "\n",
    "class StreetViewDataset(Dataset):\n",
    "    def __init__(self, csv_path, images_dir, transform=None, is_test=False):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.directions = ['image_north', 'image_east', 'image_south', 'image_west']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        images = []\n",
    "        for d in self.directions:\n",
    "            path = os.path.join(self.images_dir, row[d])\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "            except:\n",
    "                img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "        \n",
    "        img_tensor = torch.stack(images) # [4, 3, 224, 224]\n",
    "        \n",
    "        if self.is_test:\n",
    "            return img_tensor, row['sample_id']\n",
    "        \n",
    "        return img_tensor, \\\n",
    "               torch.tensor(row['state_idx'], dtype=torch.long), \\\n",
    "               torch.tensor(row['latitude'], dtype=torch.float32), \\\n",
    "               torch.tensor(row['longitude'], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db18fc",
   "metadata": {},
   "source": [
    "For this part, we design a single model that performs both state classification and GPS prediction by fusing information from multiple views and multiple visual backbones.\n",
    "\n",
    "We use two pretrained vision encoders. StreetCLIP is chosen because it is trained on street-level imagery and captures location-specific visual cues such as road structure, signage, and environment. DINOv3 is added to provide strong general-purpose visual features that complement StreetCLIP and improve robustness across diverse scenes.\n",
    "\n",
    "Each directional image is passed through both backbones independently. The output features from StreetCLIP and DINOv3 are projected into a common dimensionality and concatenated. We apply layer normalization after concatenation to stabilize training and align the combined feature scale.\n",
    "\n",
    "The four directional feature vectors are then reshaped into a sequence and passed through a Transformer encoder. We use a Transformer because different directions contribute differently depending on the scene, and the model should learn how to weigh and relate these views instead of treating them equally.\n",
    "\n",
    "After fusion, the features from all directions are flattened into a single global representation for the location. This representation is shared by two prediction heads. The state head outputs logits over all states for classification, while the GPS head predicts normalized latitude and longitude values for regression.\n",
    "\n",
    "The forward method also supports returning the fused feature representation directly. This is used later for nearest neighbor based GPS refinement, where the model acts as a learned embedding extractor rather than only a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97667d5-464d-4360-b319-02e6d015a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MODEL ARCHITECTURE\n",
    "# ==========================================\n",
    "class GeoFusionModel(nn.Module):\n",
    "    def __init__(self, num_states=50, fusion_dim=512):\n",
    "        super().__init__()\n",
    "        # Backbones\n",
    "        self.clip = CLIPVisionModel.from_pretrained(Config.MODEL_NAME_CLIP)\n",
    "        self.dino = AutoModel.from_pretrained(Config.MODEL_NAME_DINO)\n",
    "        \n",
    "        # Projections\n",
    "        self.clip_proj = nn.Linear(self.clip.config.hidden_size, fusion_dim)\n",
    "        self.dino_proj = nn.Linear(self.dino.config.hidden_size, fusion_dim)\n",
    "        self.norm = nn.LayerNorm(fusion_dim * 2)\n",
    "        \n",
    "        # Fusion\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=fusion_dim*2, nhead=8, batch_first=True)\n",
    "        self.fusion = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # Heads\n",
    "        self.state_head = nn.Linear(fusion_dim*2*4, num_states)\n",
    "        self.gps_head = nn.Linear(fusion_dim*2*4, 2)\n",
    "\n",
    "    def forward(self, x, return_feats = False):\n",
    "        B, N, C, H, W = x.shape\n",
    "        x_flat = x.view(B*N, C, H, W)\n",
    "        \n",
    "        # Extract\n",
    "        clip_feat = self.clip_proj(self.clip(x_flat).pooler_output)\n",
    "        dino_feat = self.dino_proj(self.dino(x_flat).last_hidden_state[:, 0, :])\n",
    "        \n",
    "        # Fuse\n",
    "        feat = torch.cat([clip_feat, dino_feat], dim=1) # [B*4, fusion_dim*2]\n",
    "        feat = self.norm(feat)\n",
    "        feat = feat.view(B, N, -1) # [B, 4, dim]\n",
    "        \n",
    "        # Transformer\n",
    "        feat = self.fusion(feat)\n",
    "        \n",
    "        # Flatten and Predict\n",
    "        global_feat = feat.reshape(B, -1)\n",
    "\n",
    "        if return_feats:\n",
    "            # Return logits, dummy_gps, features\n",
    "            return self.state_head(global_feat), self.gps_head(global_feat), global_feat\n",
    "        \n",
    "        return self.state_head(global_feat), self.gps_head(global_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3e408",
   "metadata": {},
   "source": [
    "For this part, we define the training logic for a single epoch.\n",
    "\n",
    "The model is set to training mode so that layers such as dropout and normalization behave correctly. We track three metrics during training: total loss, state classification accuracy, and average Haversine distance in kilometers. These give both optimization and task-level feedback.\n",
    "\n",
    "We use cross-entropy loss for state classification and mean squared error loss for GPS regression. The two losses are combined using configurable weights. This allows us to balance coarse location recognition through state prediction with fine-grained geographic precision through GPS regression.\n",
    "\n",
    "Before computing the GPS loss, latitude and longitude targets are normalized. This keeps the regression targets on a similar scale and prevents one coordinate from dominating the loss.\n",
    "\n",
    "Mixed precision training is used during the forward pass to improve training efficiency on GPU. Gradients are scaled to avoid numerical underflow, and gradient clipping is applied to stabilize updates when fine-tuning large pretrained backbones.\n",
    "\n",
    "After each update, we compute classification accuracy and convert the predicted GPS values back to real coordinates. The Haversine distance between predictions and ground truth is then calculated to measure real-world localization error.\n",
    "\n",
    "All metrics are accumulated across the epoch and averaged at the end, providing a concise summary of training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f534726-8d4f-497f-8b8f-8571d03ad683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TRAINING & VALIDATION\n",
    "# ==========================================\n",
    "def train_epoch(model, loader, optimizer, scaler, normalizer):\n",
    "    model.train()\n",
    "    meters = {'loss': 0, 'acc': 0, 'dist': 0}\n",
    "    \n",
    "    criterion_cls = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    criterion_gps = nn.MSELoss()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for imgs, states, lats, lons in pbar:\n",
    "        imgs, states = imgs.to(Config.DEVICE), states.to(Config.DEVICE)\n",
    "        lats, lons = lats.to(Config.DEVICE), lons.to(Config.DEVICE)\n",
    "        \n",
    "        # Normalize targets\n",
    "        n_lat, n_lon = normalizer.normalize(lats, lons)\n",
    "        gps_targets = torch.stack([n_lat, n_lon], dim=1)\n",
    "        \n",
    "        # Mixed Precision Forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits, gps_preds = model(imgs)\n",
    "            loss = (Config.W_CLS * criterion_cls(logits, states)) + \\\n",
    "                   (Config.W_GPS * criterion_gps(gps_preds, gps_targets))\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Metrics\n",
    "        acc = (logits.argmax(1) == states).float().mean()\n",
    "        p_lat, p_lon = normalizer.denormalize(gps_preds[:,0], gps_preds[:,1])\n",
    "        dist = haversine_distance(p_lat, p_lon, lats, lons).mean()\n",
    "        \n",
    "        meters['loss'] += loss.item()\n",
    "        meters['acc'] += acc.item()\n",
    "        meters['dist'] += dist.item()\n",
    "        pbar.set_postfix({'Loss': loss.item(), 'Acc': acc.item(), 'km': dist.item()})\n",
    "        \n",
    "    return {k: v/len(loader) for k, v in meters.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415adb3",
   "metadata": {},
   "source": [
    "For this part, we put together all components required to train the model end to end.\n",
    "\n",
    "We first initialize the GPS normalizer and define the image preprocessing pipeline. All images are resized to a fixed resolution and normalized using standard ImageNet statistics. This ensures compatibility with the pretrained vision backbones and keeps the input distribution stable.\n",
    "\n",
    "The full training dataset is then split into training and validation subsets. We use a 90 to 10 split to retain most of the data for learning while still having a small holdout set for monitoring training behavior.\n",
    "\n",
    "Next, we initialize the GeoFusion model and move it to the appropriate device. We use the AdamW optimizer because it works well for fine-tuning large pretrained models and handles weight decay in a stable manner. Mixed precision training is enabled through a gradient scaler to improve efficiency.\n",
    "\n",
    "Before starting training, we check whether a saved checkpoint is available. If so, we restore the model and optimizer states and continue training from the last saved epoch. This allows long training runs to be resumed without losing progress.\n",
    "\n",
    "The training loop then runs for the specified number of epochs. In each epoch, we call the training function to update model parameters and compute training metrics. After every epoch, a full checkpoint is saved as a safety measure.\n",
    "\n",
    "If the current epoch achieves a lower training loss than any previous epoch, the model is saved as the best version. Both a full checkpoint and a lightweight weights-only file are stored to support later inference and submission generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960f483-1787-4e88-991b-4640a854ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models: geolocal/StreetCLIP & facebook/dinov3-vitl16-pretrain-lvd1689m...\n",
      "Resuming training from best_model_full.pth...\n",
      "Full checkpoint loaded! Resuming from Epoch 30\n",
      "Starting Training from Epoch 31 to 35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14846/14846 [4:34:05<00:00,  1.11s/it, Loss=0.00581, Acc=1, km=55.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Train: {'loss': 0.010913128783051975, 'acc': 0.9999831604472585, 'dist': 67.92319890962334}\n",
      "Saved Best Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14846/14846 [4:34:34<00:00,  1.11s/it, Loss=0.00313, Acc=1, km=37.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Train: {'loss': 0.010695938372213675, 'acc': 0.9999831604472585, 'dist': 67.37325995292494}\n",
      "Saved Best Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14846/14846 [4:34:15<00:00,  1.11s/it, Loss=0.0164, Acc=1, km=97.3]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Train: {'loss': 0.010437070277856427, 'acc': 0.9999831604472585, 'dist': 66.83835264487551}\n",
      "Saved Best Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14846/14846 [4:34:08<00:00,  1.11s/it, Loss=0.0103, Acc=1, km=62.7]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Train: {'loss': 0.010300078411381464, 'acc': 0.9999831604472585, 'dist': 66.2753403393965}\n",
      "Saved Best Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14846/14846 [4:34:13<00:00,  1.11s/it, Loss=0.00826, Acc=1, km=76.9]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Train: {'loss': 0.0100834660223512, 'acc': 0.9999831604472585, 'dist': 65.7511261384731}\n",
      "Saved Best Model!\n"
     ]
    }
   ],
   "source": [
    "# 5. MAIN TRAINING\n",
    "# ==========================================\n",
    "\n",
    "# Setup\n",
    "normalizer = GPSNormalizer()\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Data (Assuming CSVs exist - Split Train for Val)\n",
    "full_ds = StreetViewDataset(Config.TRAIN_CSV, Config.TRAIN_IMG_DIR, transform=tfm)\n",
    "train_size = int(0.9 * len(full_ds))\n",
    "val_size = len(full_ds) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
    "val_loader = DataLoader(val_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=Config.NUM_WORKERS)\n",
    "\n",
    "# Init Model\n",
    "print(f\"Loading Models: {Config.MODEL_NAME_CLIP} & {Config.MODEL_NAME_DINO}...\")\n",
    "model = GeoFusionModel().to(Config.DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=Config.LR, weight_decay=Config.WD)\n",
    "scaler = torch.cuda.amp.GradScaler() # For Mixed Precision\n",
    " \n",
    "# Resume Logic\n",
    "start_epoch = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "if Config.RESUME_PATH and os.path.exists(Config.RESUME_PATH):\n",
    "    print(f\"Resuming training from {Config.RESUME_PATH}...\")\n",
    "    checkpoint = torch.load(Config.RESUME_PATH, map_location=Config.DEVICE)\n",
    "    \n",
    "    # Scenario A: You saved a Full Checkpoint (Model + Optimizer + Epoch)\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "        print(f\"Full checkpoint loaded! Resuming from Epoch {start_epoch}\")\n",
    "\n",
    "    # FORCE LEARNING RATE OVERRIDE (Optional)\n",
    "        # Explicitly overwrite the loaded LR with the one in the current Config\n",
    "        # for param_group in optimizer.param_groups:\n",
    "        #     param_group['lr'] = Config.LR\n",
    "            \n",
    "        # print(f\"Optimizer learning rate successfully updated to: {Config.LR}\")\n",
    "        \n",
    "    # Scenario B: You only saved Weights\n",
    "    else:\n",
    "        # If checkpoint is just the state_dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"Weights loaded successfully! (Optimizer reset because file contained weights only)\")\n",
    "\n",
    "\n",
    "# Training Loop (Adjusted Range)\n",
    "print(f\"Starting Training from Epoch {start_epoch+1} to {Config.EPOCHS}...\")\n",
    "\n",
    "for epoch in range(start_epoch, Config.EPOCHS):\n",
    "    train_metrics = train_epoch(model, train_loader, optimizer, scaler, normalizer)\n",
    "    print(f\"Epoch {epoch+1} Train: {train_metrics}\")\n",
    "    \n",
    "    # IMPROVED SAVING LOGIC \n",
    "    # Save a Full Checkpoint, so you can resume perfectly next time\n",
    "    checkpoint_state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "    }\n",
    "    \n",
    "    # Save latest checkpoint every epoch (safety net)\n",
    "    torch.save(checkpoint_state, os.path.join(Config.SAVE_DIR, \"last_checkpoint.pth\"))\n",
    "    \n",
    "    # Save best model if loss improved\n",
    "    if train_metrics['loss'] < best_loss:\n",
    "        best_loss = train_metrics['loss']\n",
    "        checkpoint_state['best_loss'] = best_loss # Update best loss in dict\n",
    "        torch.save(checkpoint_state, \"best_model_full.pth\")\n",
    "        # Also save just weights for inference script compatibility\n",
    "        torch.save(model.state_dict(), \"best_model.pth\") \n",
    "        print(\"Saved Best Model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e779009",
   "metadata": {},
   "source": [
    "For this part, we switch the model from training to inference mode and prepare the components needed for GPS refinement using nearest neighbors.\n",
    "\n",
    "We load the best-performing model weights and set the model to evaluation mode. From this point onward, the model is used only for forward passes and feature extraction, with gradients disabled to reduce memory usage and improve speed.\n",
    "\n",
    "Before running inference on the test set, we construct a knowledge base of embeddings from the training data. Each training sample is passed through the model to extract the fused feature representation learned during training. These embeddings act as reference points for nearest neighbor based GPS estimation.\n",
    "\n",
    "To improve robustness, we apply five-crop test-time augmentation when extracting training embeddings. The original image and five cropped versions are all passed through the model, and the resulting feature vectors are averaged. This reduces sensitivity to framing and local viewpoint changes while keeping the representation stable.\n",
    "\n",
    "All averaged embeddings are L2-normalized so that cosine similarity can be used reliably during nearest neighbor search. Along with the embeddings, we store the corresponding ground truth GPS coordinates.\n",
    "\n",
    "Because this embedding extraction step is computationally expensive, the results are cached to disk. If cached embeddings already exist, they are loaded directly, avoiding redundant computation when rerunning inference or tuning K-NN parameters.\n",
    "\n",
    "This embedding database forms the foundation for the subsequent K-NN based GPS refinement step used during test-time prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cc046-900e-4c14-8411-94e81c5ac5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Inference...\n",
      "Cache missing! Generating training embeddings first (Required for K-NN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Train Set:   5%|▌         | 829/16495 [1:34:01<29:38:11,  6.81s/it]"
     ]
    }
   ],
   "source": [
    "# 6. INFERENCE & SUBMISSION (Integrated K-NN)\n",
    "# ==========================================\n",
    "print(\"Starting Inference...\")\n",
    "\n",
    "# Load Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.to(Config.DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# STEP A: PREPARE THE \"KNOWLEDGE BASE\" (Training Embeddings)\n",
    "# ---------------------------------------------------------\n",
    "TRAIN_EMB_FILE = \"train_embeddings.npy\"\n",
    "TRAIN_GPS_FILE = \"train_gps.npy\"\n",
    "\n",
    "if os.path.exists(TRAIN_EMB_FILE) and os.path.exists(TRAIN_GPS_FILE):\n",
    "    print(\"Loading cached training embeddings...\")\n",
    "    train_emb = np.load(TRAIN_EMB_FILE)\n",
    "    train_gps = np.load(TRAIN_GPS_FILE)\n",
    "else:\n",
    "    print(\"Cache missing! Generating training embeddings first (Required for K-NN)...\")\n",
    "    # Quick loop to generate training embeddings if they don't exist\n",
    "    train_ds = StreetViewDataset(Config.TRAIN_CSV, Config.TRAIN_IMG_DIR, transform=tfm)\n",
    "    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=Config.NUM_WORKERS)\n",
    "    \n",
    "    emb_list, gps_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _, lat, lon in tqdm(train_loader, desc=\"Indexing Train Set\"):\n",
    "            imgs = imgs.to(Config.DEVICE)\n",
    "            # Use return_feats=True to get the vector\n",
    "            _, _, feats = model(imgs, return_feats=True)\n",
    "\n",
    "            # Five-Crop TTA\n",
    "            B, N, C, H, W = imgs.shape\n",
    "        \n",
    "            # We treat the 4 sub-images (N/E/S/W) independently for cropping\n",
    "            # Reshape to [B*4, C, H, W] to crop all directions at once\n",
    "            flat_imgs = imgs.view(B*N, C, H, W)\n",
    "            \n",
    "            # Crop size: 80% of image (approx 268px)\n",
    "            crop_size = int(H * 0.8) \n",
    "            \n",
    "            # Get 5 crops: TL, TR, BL, BR, Center\n",
    "            crops = transforms.FiveCrop(crop_size)(flat_imgs) \n",
    "            # crops is a tuple of 5 tensors, each [B*N, C, crop_h, crop_w]\n",
    "            \n",
    "            tta_feats_sum = feats.clone()\n",
    "            \n",
    "            for crop in crops:\n",
    "                # Resize back to 336x336 so model accepts it\n",
    "                crop_resized = F.resize(crop, [H, W])\n",
    "                # Reshape back to [B, 4, C, H, W]\n",
    "                crop_reshaped = crop_resized.view(B, N, C, H, W)\n",
    "                \n",
    "                _, _, feats_crop = model(crop_reshaped, return_feats=True)\n",
    "                tta_feats_sum += feats_crop\n",
    "            \n",
    "            # Average all 6 views (Original + 5 Crops)\n",
    "            feats_avg = tta_feats_sum / 6.0\n",
    "            \n",
    "            # Renormalize! (Crucial for Cosine Similarity)\n",
    "            feats_avg = torch.nn.functional.normalize(feats_avg, p=2, dim=1)\n",
    "\n",
    "            emb_list.append(feats_avg.cpu().numpy())\n",
    "            gps_list.append(torch.stack([lat, lon], dim=1).numpy())\n",
    "            \n",
    "    train_emb = np.concatenate(emb_list)\n",
    "    train_gps = np.concatenate(gps_list)\n",
    "    \n",
    "    # Save for next time\n",
    "    np.save(TRAIN_EMB_FILE, train_emb)\n",
    "    np.save(TRAIN_GPS_FILE, train_gps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94595237-003d-4665-8194-2172ca776ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP B: INFERENCE LOOP (Collect Data)\n",
    "# ---------------------------------------------------------\n",
    "test_ds = StreetViewDataset(Config.TEST_CSV, Config.TEST_IMG_DIR, transform=tfm, is_test=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=Config.NUM_WORKERS)\n",
    "\n",
    "# We will store results in lists first, then process K-NN in one fast batch\n",
    "all_sample_ids = []\n",
    "all_test_embeddings = []\n",
    "all_top5_states = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, sample_ids in tqdm(test_loader, desc=\"Predicting Test Set\"):\n",
    "        imgs = imgs.to(Config.DEVICE)\n",
    "        \n",
    "        # 1. Run Model\n",
    "        # We need both logits (for state) and feats (for K-NN GPS)\n",
    "        # We ignore the model's raw 'gps_preds' because we are using K-NN instead\n",
    "        logits, _, feats = model.forward(imgs, return_feats=True)\n",
    "\n",
    "        # Five-Crop TTA\n",
    "        B, N, C, H, W = imgs.shape\n",
    "        \n",
    "        # We treat the 4 sub-images (N/E/S/W) independently for cropping\n",
    "        # Reshape to [B*4, C, H, W] to crop all directions at once\n",
    "        flat_imgs = imgs.view(B*N, C, H, W)\n",
    "        \n",
    "        # Crop size: 80% of image (approx 268px)\n",
    "        crop_size = int(H * 0.8) \n",
    "        \n",
    "        # Get 5 crops: TL, TR, BL, BR, Center\n",
    "        crops = transforms.FiveCrop(crop_size)(flat_imgs) \n",
    "        # crops is a tuple of 5 tensors, each [B*N, C, crop_h, crop_w]\n",
    "        \n",
    "        tta_feats_sum = feats.clone()\n",
    "        \n",
    "        for crop in crops:\n",
    "            # Resize back to 336x336 so model accepts it\n",
    "            crop_resized = F.resize(crop, [H, W])\n",
    "            # Reshape back to [B, 4, C, H, W]\n",
    "            crop_reshaped = crop_resized.view(B, N, C, H, W)\n",
    "            \n",
    "            _, _, feats_crop = model(crop_reshaped, return_feats=True)\n",
    "            tta_feats_sum += feats_crop\n",
    "        \n",
    "        # Average all 6 views (Original + 5 Crops)\n",
    "        feats_avg = tta_feats_sum / 6.0\n",
    "        \n",
    "        # Renormalize! (Crucial for Cosine Similarity)\n",
    "        feats_avg = torch.nn.functional.normalize(feats_avg, p=2, dim=1)\n",
    "        \n",
    "        # 2. Process States (Top 5)\n",
    "        _, top5_indices = torch.topk(logits, 5, dim=1)\n",
    "        \n",
    "        # 3. Store Data\n",
    "        all_test_embeddings.append(feats_avg.cpu().numpy())\n",
    "        all_top5_states.append(top5_indices.cpu().numpy())\n",
    "        all_sample_ids.extend(sample_ids.tolist())\n",
    "\n",
    "# Concatenate all test embeddings into one big matrix\n",
    "all_test_embeddings = np.concatenate(all_test_embeddings) # Shape: [N_test, 1024]\n",
    "all_top5_states = np.concatenate(all_top5_states)         # Shape: [N_test, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec04573-f13c-492d-a6df-4a6910e71348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# GRID SEARCH TO FIND OPTIMAL K AND TEMP FOR K-NN\n",
    "\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# Paths to your cached embedding/gps files\n",
    "EMB_FILE = \"train_embeddings.npy\"\n",
    "GPS_FILE = \"train_gps.npy\"\n",
    "\n",
    "# Hyperparameter Grid to Search\n",
    "K_VALUES = [5, 10, 15, 20, 25, 30, 40, 50, 75, 100]\n",
    "TEMP_VALUES = [0.5, 1, 2, 5, 10, 15, 20, 30, 50]\n",
    "\n",
    "# UTILS\n",
    "# ==========================================\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Vectorized Haversine Distance (Numpy)\n",
    "    \"\"\"\n",
    "    R = 6371\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    \n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# MAIN EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Loading cached embeddings from {EMB_FILE}...\")\n",
    "    try:\n",
    "        X = np.load(EMB_FILE)\n",
    "        y = np.load(GPS_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: train_embeddings.npy not found. Run main.py first to generate them!\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data Loaded. Shape: {X.shape}\")\n",
    "    \n",
    "    # Create a Validation Split\n",
    "    # We pretend the 'val' set is our test set to measure performance\n",
    "    print(\"Splitting data (80% Train / 20% Validation)...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=25)\n",
    "    \n",
    "    # Pre-compute Neighbors\n",
    "    # We calculate the max K once to save time, then slice it for smaller K values\n",
    "    max_k = max(K_VALUES)\n",
    "    print(f\"Fitting NN model (finding top {max_k} neighbors)...\")\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=max_k, metric='cosine', n_jobs=-1)\n",
    "    knn.fit(X_train)\n",
    "    \n",
    "    # Get distances and indices for the validation set\n",
    "    # This is the heavy lifting; done only once\n",
    "    dists_all, indices_all = knn.kneighbors(X_val)\n",
    "    \n",
    "    # Grid Search Loop\n",
    "    print(\"\\nStarting Grid Search...\")\n",
    "    print(f\"{'K':<5} | {'Temp':<5} | {'Avg Error (km)':<15}\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_params = {}\n",
    "    \n",
    "    # We iterate through K and Temperature to find the \"Goldilocks\" zone\n",
    "    for k in K_VALUES:\n",
    "        # Slice the pre-computed arrays to simulate using only 'k' neighbors\n",
    "        dists_k = dists_all[:, :k]\n",
    "        indices_k = indices_all[:, :k]\n",
    "        \n",
    "        for temp in TEMP_VALUES:\n",
    "            # WEIGHTING LOGIC \n",
    "            # Similarity = 1 - Cosine Distance\n",
    "            # High Temp = Sharp peaks (Only closest neighbor matters)\n",
    "            # Low Temp = Flat peaks (All k neighbors matter equally)\n",
    "            similarities = 1 - dists_k\n",
    "            weights = np.exp(similarities * temp)\n",
    "            \n",
    "            # Normalize weights to sum to 1 (Add epsilon to avoid division by zero)\n",
    "            weights_sum = np.sum(weights, axis=1, keepdims=True) + 1e-10\n",
    "            weights_norm = weights / weights_sum\n",
    "            \n",
    "            # PREDICTION\n",
    "            # Gather neighbor GPS coords\n",
    "            # shape: [N_val, k, 2]\n",
    "            neighbor_gps = y_train[indices_k]\n",
    "            \n",
    "            # Weighted Average\n",
    "            # Multiply weights (N, k, 1) * gps (N, k, 2) -> sum over k -> (N, 2)\n",
    "            weights_norm_exp = np.expand_dims(weights_norm, axis=2)\n",
    "            pred_gps = np.sum(weights_norm_exp * neighbor_gps, axis=1)\n",
    "            \n",
    "            # SCORING\n",
    "            errors = haversine_np(pred_gps[:,0], pred_gps[:,1], y_val[:,0], y_val[:,1])\n",
    "            mean_error = np.mean(errors)\n",
    "            \n",
    "            print(f\"{k:<5} | {temp:<5} | {mean_error:.4f} km\")\n",
    "            \n",
    "            if mean_error < best_score:\n",
    "                best_score = mean_error\n",
    "                best_params = {'k': k, 'temp': temp}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*35)\n",
    "    print(\"GRID SEARCH RESULT\")\n",
    "    print(\"=\"*35)\n",
    "    print(f\"Best K: {best_params['k']}\")\n",
    "    print(f\"Best Temperature: {best_params['temp']}\")\n",
    "    print(f\"Validation Error: {best_score:.4f} km\")\n",
    "    print(\"=\"*35)\n",
    "    print(\"\\nUpdate your main.py with these values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac90a0b-246b-4671-800d-7bc534ad3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP C: BATCH K-NN REFINEMENT\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "K_OPTIMAL = 5\n",
    "TEMP_OPTIMAL = 50\n",
    "\n",
    "print(f\"Refining GPS with K-NN (K={K_OPTIMAL}, Temp={TEMP_OPTIMAL})...\")\n",
    "\n",
    "# Fit K-NN on the training data\n",
    "print(\"Fitting K-NN model...\")\n",
    "# knn = NearestNeighbors(n_neighbors=5, metric='cosine', n_jobs=-1)\n",
    "knn = NearestNeighbors(n_neighbors=K_OPTIMAL, metric='cosine', n_jobs=-1)\n",
    "knn.fit(train_emb)\n",
    "\n",
    "distances, indices = knn.kneighbors(all_test_embeddings)\n",
    "\n",
    "refined_results = []\n",
    "\n",
    "for i in range(len(all_sample_ids)):\n",
    "    # Calculate Weighted GPS\n",
    "    neighbor_indices = indices[i]\n",
    "    neighbor_dists = distances[i]\n",
    "    neighbor_gps = train_gps[neighbor_indices]\n",
    "    \n",
    "    # Similarity = 1 - Cosine Distance\n",
    "    # Exponentiate to sharpen the weights (gives more power to very close matches)\n",
    "    weights = np.exp((1 - neighbor_dists) * TEMP_OPTIMAL) \n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    w_lat = np.sum(neighbor_gps[:, 0] * weights)\n",
    "    w_lon = np.sum(neighbor_gps[:, 1] * weights)\n",
    "    \n",
    "    # Get State Predictions (Already computed)\n",
    "    states = all_top5_states[i]\n",
    "    \n",
    "    # Create Row\n",
    "    row = {\n",
    "        'sample_id': all_sample_ids[i],\n",
    "        'predicted_state_idx_1': states[0],\n",
    "        'predicted_state_idx_2': states[1],\n",
    "        'predicted_state_idx_3': states[2],\n",
    "        'predicted_state_idx_4': states[3],\n",
    "        'predicted_state_idx_5': states[4],\n",
    "        'predicted_latitude': w_lat,   # <--- K-NN Refined\n",
    "        'predicted_longitude': w_lon   # <--- K-NN Refined\n",
    "    }\n",
    "    refined_results.append(row)\n",
    "\n",
    "# STEP D: FORMATTING & SAVING\n",
    "# ---------------------------------------------------------\n",
    "df_sub = pd.DataFrame(refined_results)\n",
    "\n",
    "# Template Merge (Best Practice for Kaggle to ensure order/columns)\n",
    "template = pd.read_csv(Config.TEST_CSV)\n",
    "final_df = template.merge(df_sub, on='sample_id', how='left', suffixes=('', '_pred'))\n",
    "\n",
    "# Overwrite columns\n",
    "final_df['predicted_state_idx_1'] = final_df['predicted_state_idx_1_pred']\n",
    "final_df['predicted_latitude'] = final_df['predicted_latitude_pred']\n",
    "final_df['predicted_longitude'] = final_df['predicted_longitude_pred']\n",
    "\n",
    "# Optional columns\n",
    "for k in range(2, 6):\n",
    "    col_name = f'predicted_state_idx_{k}'\n",
    "    if f'{col_name}_pred' in final_df.columns:\n",
    "        final_df[col_name] = final_df[f'{col_name}_pred']\n",
    "\n",
    "# Save\n",
    "final_df.to_csv(Config.SUBMISSION_FILE, index=False)\n",
    "print(f\"Submission saved to {Config.SUBMISSION_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
